{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0bc4876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcribing file: \n",
      "/Users/ayserchoudhury/playground/jarvis/transcribe/recordings/sample.wav\n"
     ]
    }
   ],
   "source": [
    "import setup, os, json\n",
    "from recordings.audio_file_path import audio_file_path,transcript_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c205b5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayserchoudhury/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiating transcription pipeline..\n"
     ]
    }
   ],
   "source": [
    "# Setting up transcriber\n",
    "from transformers import pipeline\n",
    "print(\"initiating transcription pipeline..\")\n",
    "transcriber = pipeline(\"automatic-speech-recognition\",model=\"openai/whisper-large-v3\",max_new_tokens=100, chunk_length_s=30)\n",
    "print(\"transcription pipelines ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b1f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribing full audio with timestamps\n",
    "print(\"transcribing audio...\")\n",
    "full_transcription = transcriber(audio_file_path,max_new_tokens=256,generate_kwargs={\"task\": \"transcribe\"},chunk_length_s=30,return_timestamps=True)\n",
    "print(\"completed transcription.\")\n",
    "\n",
    "# print(full_transcription)\n",
    "\n",
    "# chunks = full_transcription[\"chunks\"]\n",
    "# for c in chunks:\n",
    "#     print(c)\n",
    "\n",
    "full_transcription[\"chunks\"][1][\"timestamp\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c7a134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayserchoudhury/anaconda3/lib/python3.11/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing diarization pipeline...\n",
      "diarizing audio..\n",
      "completed diarization.\n"
     ]
    }
   ],
   "source": [
    "# Setting up diarizer \n",
    "from pyannote.audio import Pipeline\n",
    "from keys import pyannote_key\n",
    "\n",
    "print(\"initializing diarization pipeline...\")\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=pyannote_key)\n",
    "print(\"diarization pipeline ready.\")\n",
    "print(\"diarizing audio..\")\n",
    "diarization = pipeline(audio_file_path)\n",
    "\n",
    "d_starts = []\n",
    "d_ends = []\n",
    "d_speakers = []\n",
    "\n",
    "for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    d_starts.append(segment.start)\n",
    "    d_ends.append(segment.end)\n",
    "    d_speakers.append(speaker)\n",
    "    \n",
    "print(\"completed diarization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97ee50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPEAKER_00', 'SPEAKER_01', 'SPEAKER_00', 'SPEAKER_00', 'SPEAKER_01', 'SPEAKER_00', 'SPEAKER_01', 'SPEAKER_00', 'SPEAKER_00', 'SPEAKER_01', 'SPEAKER_00', 'SPEAKER_01']\n",
      "[0.6536502546689305, 4.473684210526315, 7.427843803056028, 9.872665534804755, 11.621392190152802, 14.779286926994908, 20.551782682512737, 21.7572156196944, 24.01528013582343, 26.137521222410868, 28.599320882852297, 33.74363327674024]\n",
      "[4.1341256366723265, 6.867572156196944, 9.617996604414262, 11.451612903225808, 13.964346349745332, 20.28013582342954, 23.132427843803057, 21.7911714770798, 25.916808149405774, 28.07300509337861, 33.217317487266556, 36.44312393887946]\n"
     ]
    }
   ],
   "source": [
    "print(d_speakers)\n",
    "print(d_starts)\n",
    "print(d_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c089bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (860803,), Data type: float32, Sampling rate: 22050\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "y, sr = librosa.load(audio_file_path)\n",
    "\n",
    "# Convert to desired data type (float32 or float64)\n",
    "audio_array = y.astype(np.float32)  # or np.float64\n",
    "\n",
    "# Ensure it's 1D array\n",
    "if audio_array.ndim > 1:\n",
    "    audio_array = audio_array.flatten()\n",
    "\n",
    "print(f\"Shape: {audio_array.shape}, Data type: {audio_array.dtype}, Sampling rate: {}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd284277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "860803\n",
      "[ 1.1343518e-06 -1.8206911e-06 -2.1902491e-05 -2.0330670e-05\n",
      " -1.6933725e-05]\n",
      "36.44312393887946\n"
     ]
    }
   ],
   "source": [
    "print(len(audio_array))\n",
    "\n",
    "# multiplier = len(audio_array)/d_ends[len(d_ends)-1]\n",
    "print(audio_array[:5])\n",
    "print(d_ends[len(d_ends)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4a50c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "860803\n",
      "[ 3.4618728e-11 -5.5564778e-11 -6.6843137e-10 -6.2046174e-10\n",
      " -5.1679205e-10]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "processed_data = np.array(audio_array).astype(np.float32) / 32767.0\n",
    "print(len(processed_data))\n",
    "print(processed_data[:5])\n",
    "print(type(processed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a18e2969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' you you'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber({\"sampling_rate\": sr, \"raw\": processed_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c1b36ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' you you'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "print(multiplier)\n",
    "\n",
    "diarized_transcription = \"\"\n",
    "diarized_transcription += d_speakers[i]+ \": \"+ transcription['text'] + \"\\n\"\n",
    "\n",
    "print(len(audio_array))\n",
    "for i in range(0,len(d_ends)):\n",
    "    print(x)\n",
    "    if (int(d_ends[i] * multiplier)>len(audio_array)):\n",
    "        transcription = transcriber(audio_array[x:len(audio_array)-1],generate_kwargs={\"task\": \"transcribe\"})\n",
    "        diarized_transcription += d_speakers[i]+ \": \"+ transcription['text'] + \"\\n\"\n",
    "    else:\n",
    "        y = int(d_ends[i] * multiplier)\n",
    "        transcription = transcriber(audio_array[x:y],generate_kwargs={\"task\": \"transcribe\"})\n",
    "    diarized_transcription += d_speakers[i]+ \": \"+ transcription['text'] + \"\\n\"\n",
    "    x = y\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarized_transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c50f1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454bca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1403f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load audio file\n",
    "audio = AudioSegment.from_wav(audio_file_path)\n",
    "print(len(audio))\n",
    "diarized_transcription = \"\"\n",
    "\n",
    "# Split and save chunks\n",
    "for i, timestamp in enumerate(d_ends):\n",
    "    start, end = timestamp, d_ends[i + 1] if i + 1 < len(d_ends) else None\n",
    "#     print(start)\n",
    "#     print(end)\n",
    "    segment = audio[start:end]\n",
    "    print(end-start)\n",
    "#     segment.export(f\"segment_{i + 1}.wav\", format=\"wav\")\n",
    "    transcription = transcriber(f\"segment_{i + 1}.wav\",max_new_tokens=256,generate_kwargs={\"task\": \"transcribe\"},chunk_length_s=30,return_timestamps=True)\n",
    "    diarized_transcription += d_speakers[i]+ \": \"+ transcription['text'] + \"\\n\"\n",
    "#     print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diarized_transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e09cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Load audio\n",
    "y, sr = librosa.load(audio_file_path)\n",
    "diarized_transcription = \"\"\n",
    "\n",
    "# Define timestamps (in seconds)\n",
    "timestamps = d_ends\n",
    "\n",
    "# Convert timestamps to sample indices\n",
    "sample_indices = [int(timestamp * sr) for timestamp in timestamps]\n",
    "\n",
    "# Split and save chunks\n",
    "for i, start_index in enumerate(sample_indices):\n",
    "    end_index = sample_indices[i + 1] if i + 1 < len(sample_indices) else len(y)\n",
    "    audio_chunk = y[start_index:end_index]\n",
    "    transcription = transcriber(audio_chunk,max_new_tokens=256,generate_kwargs={\"task\": \"transcribe\"},chunk_length_s=30,return_timestamps=True)\n",
    "    diarized_transcription += d_speakers[i]+ \": \"+ transcription['text'] + \"\\n\"\n",
    "    # Save or process the audio chunk using appropriate methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0321a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diarized_transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aaf33b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
