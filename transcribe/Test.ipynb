{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82475bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcribing file: \n",
      "/Users/ayserchoudhury/playground/jarvis/transcribe/recordings/sample.wav\n",
      "Shape: (860803,), Data type: float32, Sampling rate: 22050\n"
     ]
    }
   ],
   "source": [
    "import setup, os, json\n",
    "from recordings.audio_file_path import audio_file_path,transcript_name\n",
    "import librosa\n",
    "import numpy as np\n",
    "y, sr = librosa.load(audio_file_path)\n",
    "\n",
    "# Convert to desired data type (float32 or float64)\n",
    "audio_array = y.astype(np.float32)  # or np.float64\n",
    "\n",
    "# Ensure it's 1D array\n",
    "if audio_array.ndim > 1:\n",
    "    audio_array = audio_array.flatten()\n",
    "\n",
    "print(f\"Shape: {audio_array.shape}, Data type: {audio_array.dtype}, Sampling rate: {sr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdf7469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayserchoudhury/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/ayserchoudhury/anaconda3/lib/python3.11/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"automatic-speech-recognition\",model=\"openai/whisper-large-v3\",max_new_tokens=100, chunk_length_s=30)\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "from keys import pyannote_key\n",
    "\n",
    "diarization_pipeline = pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=pyannote_key)\n",
    "\n",
    "\n",
    "# returns diarization info such as segment start and end times, and speaker id\n",
    "def diarization_info(res):\n",
    "    starts = []\n",
    "    ends = []\n",
    "    speakers = []\n",
    "\n",
    "    for segment, _, speaker in res.itertracks(yield_label=True):\n",
    "        starts.append(segment.start)\n",
    "        ends.append(segment.end)\n",
    "        speakers.append(speaker)\n",
    "\n",
    "    return starts, ends, speakers\n",
    "\n",
    "\n",
    "# plot diarization results on a graph\n",
    "def plot_diarization(starts, ends, speakers):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Define a color map for different speakers\n",
    "    num_speakers = len(set(speakers))\n",
    "    colors = [f\"hsl({h},80%,60%)\" for h in np.linspace(0, 360, num_speakers)]\n",
    "\n",
    "    # Plot each segment with its speaker's color\n",
    "    for start, end, speaker in zip(starts, ends, speakers):\n",
    "        speaker_id = list(set(speakers)).index(speaker)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[start, end],\n",
    "                y=[speaker_id, speaker_id],\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=colors[speaker_id], width=15),\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Speaker Diarization\",\n",
    "        xaxis=dict(title=\"Time\"),\n",
    "        yaxis=dict(title=\"Speaker\"),\n",
    "        height=600,\n",
    "        width=800,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def transcribe(sr, data):\n",
    "    processed_data = np.array(data).astype(np.float32) / 32767.0\n",
    "\n",
    "    # results from the pipeline\n",
    "    transcription_res = pipe({\"sampling_rate\": sr, \"raw\": processed_data})[\"text\"]\n",
    "\n",
    "    return transcription_res\n",
    "\n",
    "\n",
    "def transcribe_diarize(audio):\n",
    "    sr, data = audio\n",
    "    processed_data = np.array(data).astype(np.float32) / 32767.0\n",
    "    waveform_tensor = torch.tensor(processed_data[np.newaxis, :])\n",
    "\n",
    "    transcription_res = transcribe(sr, data)\n",
    "\n",
    "    # results from the diarization pipeline\n",
    "    diarization_res = diarization_pipeline(\n",
    "        {\"waveform\": waveform_tensor, \"sample_rate\": sr}\n",
    "    )\n",
    "\n",
    "    # Get diarization information\n",
    "    starts, ends, speakers = diarization_info(diarization_res)\n",
    "\n",
    "    # results from the transcription pipeline\n",
    "    diarized_transcription = \"\"\n",
    "\n",
    "    # Get transcription results for each speaker segment\n",
    "    for start_time, end_time, speaker_id in zip(starts, ends, speakers):\n",
    "        segment = data[int(start_time * sr) : int(end_time * sr)]\n",
    "        diarized_transcription += f\"{speaker_id} {round(start_time, 2)}:{round(end_time, 2)} \\t {transcribe(sr, segment)}\\n\"\n",
    "\n",
    "    # Plot diarization\n",
    "    diarization_plot = plot_diarization(starts, ends, speakers)\n",
    "\n",
    "    return transcription_res, diarized_transcription, diarization_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d191a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m transcribe_diarize(audio_array)\n",
      "Cell \u001b[0;32mIn[4], line 66\u001b[0m, in \u001b[0;36mtranscribe_diarize\u001b[0;34m(audio)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranscribe_diarize\u001b[39m(audio):\n\u001b[0;32m---> 66\u001b[0m     sr, data \u001b[38;5;241m=\u001b[39m audio\n\u001b[1;32m     67\u001b[0m     processed_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m32767.0\u001b[39m\n\u001b[1;32m     68\u001b[0m     waveform_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(processed_data[np\u001b[38;5;241m.\u001b[39mnewaxis, :])\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "x = transcribe_diarize(audio_array)\n",
    "'''\n",
    "Look into the GR.Audio file and see what the expected return is there\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x))\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
